{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2665b602",
   "metadata": {},
   "source": [
    "# Data volume check for GPU preprocessing\n",
    "\n",
    "Quickly inspect the RecSys CSV inputs to decide whether a single-GPU `cudf` flow is fine or if we need to scale out with `dask_cudf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4e2469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzed folder: /tmp\n",
      "events.csv.zip               rows=   2.76M | size=   89.87 MB\n",
      "item_properties_part1.csv.zip rows=  11.00M | size=  461.88 MB\n",
      "item_properties_part2.csv.zip rows=   9.28M | size=  389.99 MB\n",
      "category_tree.csv            rows=    1.7k | size=    0.01 MB\n",
      "------------------------------------------------------------------------\n",
      "TOTAL rows=  23.03M | size=  941.75 MB (0.92 GB)\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import zipfile\n",
    "from dataclasses import dataclass\n",
    "from typing import Iterable\n",
    "import pynvml\n",
    "\n",
    "# Data is mounted to /tmp in the rapids container\n",
    "DATA_DIR = pathlib.Path(\"/tmp\")\n",
    "\n",
    "@dataclass\n",
    "class FileStats:\n",
    "    name: str\n",
    "    rows: int\n",
    "    size_bytes: int\n",
    "\n",
    "    @property\n",
    "    def size_mb(self) -> float:\n",
    "        return self.size_bytes / (1024 ** 2)\n",
    "\n",
    "    @property\n",
    "    def size_gb(self) -> float:\n",
    "        return self.size_bytes / (1024 ** 3)\n",
    "\n",
    "\n",
    "def count_rows_in_zip(zip_path: pathlib.Path) -> tuple[int, int]:\n",
    "    \"\"\"Return (rows, uncompressed_size_bytes) for a single-file zip.\"\"\"\n",
    "    with zipfile.ZipFile(zip_path) as zf:\n",
    "        info = zf.infolist()[0]\n",
    "        with zf.open(info) as fh:\n",
    "            # Skip header, then count remaining lines\n",
    "            header = fh.readline()\n",
    "            rows = sum(1 for _ in fh)\n",
    "    return rows, info.file_size\n",
    "\n",
    "\n",
    "def count_rows_in_plain(csv_path: pathlib.Path) -> tuple[int, int]:\n",
    "    with csv_path.open(\"r\", encoding=\"utf-8\") as fh:\n",
    "        header = fh.readline()\n",
    "        rows = sum(1 for _ in fh)\n",
    "    return rows, csv_path.stat().st_size\n",
    "\n",
    "\n",
    "def humanize_rows(n: int) -> str:\n",
    "    if n >= 1_000_000:\n",
    "        return f\"{n/1_000_000:.2f}M\"\n",
    "    if n >= 1_000:\n",
    "        return f\"{n/1_000:.1f}k\"\n",
    "    return str(n)\n",
    "\n",
    "\n",
    "def summarize(stats: Iterable[FileStats]):\n",
    "    print(f\"Analyzed folder: {DATA_DIR}\")\n",
    "    total_rows = 0\n",
    "    total_bytes = 0\n",
    "    for s in stats:\n",
    "        total_rows += s.rows\n",
    "        total_bytes += s.size_bytes\n",
    "        print(f\"{s.name:28} rows={humanize_rows(s.rows):>8} | size={s.size_mb:8.2f} MB\")\n",
    "    print(\"-\" * 72)\n",
    "    print(f\"TOTAL rows={humanize_rows(total_rows):>8} | size={total_bytes/(1024**2):8.2f} MB ({total_bytes/(1024**3):.2f} GB)\")\n",
    "\n",
    "\n",
    "zip_files = [\n",
    "    DATA_DIR / \"events.csv.zip\",\n",
    "    DATA_DIR / \"item_properties_part1.csv.zip\",\n",
    "    DATA_DIR / \"item_properties_part2.csv.zip\",\n",
    "]\n",
    "plain_files = [DATA_DIR / \"category_tree.csv\"]\n",
    "\n",
    "stats: list[FileStats] = []\n",
    "for path in zip_files:\n",
    "    rows, size_bytes = count_rows_in_zip(path)\n",
    "    stats.append(FileStats(path.name, rows, size_bytes))\n",
    "\n",
    "for path in plain_files:\n",
    "    rows, size_bytes = count_rows_in_plain(path)\n",
    "    stats.append(FileStats(path.name, rows, size_bytes))\n",
    "\n",
    "summarize(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7898767",
   "metadata": {},
   "source": [
    "### CuDF vs Dask guidance\n",
    "\n",
    "- If GPU has \\u2265 ~8 GB free, the ~1 GB uncompressed data above fits comfortably in single-GPU `cudf`.\n",
    "- If you expect to duplicate DataFrames (joins, wide feature sets), or GPU memory is tight, start with `dask_cudf.read_csv` on the zip files to stream in partitions.\n",
    "- For CPU fallback, replace `cudf` with `pandas` in your preprocessing cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e660e851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**GPU 0 memory:** total=4.00 GB, free=3.62 GB\n",
       "\n",
       "**Decision:** use `dask_cudf` for preprocessing."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    pynvml.nvmlInit()\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "    mem = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "    free_gb = mem.free / 1024**3\n",
    "    total_gb = mem.total / 1024**3\n",
    "    decision = \"cudf\" if free_gb >= 8 else \"dask_cudf\"\n",
    "    print(f\"GPU 0 memory: total={total_gb:.2f} GB, free={free_gb:.2f} GB.\")\n",
    "    print(f\"Decision: use `{decision}` for preprocessing.\")\n",
    "    pynvml.nvmlShutdown()\n",
    "except Exception as e:\n",
    "    print(f\"GPU check failed: `{e}`. Default to `cudf` if GPU is available; otherwise use `pandas`.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
